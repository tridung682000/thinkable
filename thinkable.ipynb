{
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.11.11",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "kaggle": {
      "accelerator": "none",
      "dataSources": [
        {
          "sourceId": 91496,
          "databundleVersionId": 11802066,
          "sourceType": "competition"
        }
      ],
      "dockerImageVersionId": 31012,
      "isInternetEnabled": false,
      "language": "python",
      "sourceType": "notebook",
      "isGpuEnabled": false
    },
    "colab": {
      "name": "thinkable",
      "provenance": [],
      "toc_visible": true,
      "cell_execution_strategy": "setup",
      "include_colab_link": true
    }
  },
  "nbformat_minor": 0,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/tridung682000/thinkable/blob/main/thinkable.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Utils\n",
        "import random\n",
        "import matplotlib.pyplot as plt\n",
        "import torch\n",
        "import numpy as np\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data.dataset import Dataset\n",
        "import numpy as np # linear algebra\n",
        "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
        "from torchsummary import summary\n",
        "\n",
        "def start_train(train_func, *args, **kwargs):\n",
        "    for epoch in range(50):\n",
        "        running_loss = 0.0\n",
        "        print(\"epoch\", epoch+1)\n",
        "        i = 0\n",
        "        train_func(*args, **kwargs)\n"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-08-10T13:16:48.988018Z",
          "iopub.execute_input": "2025-08-10T13:16:48.988311Z",
          "iopub.status.idle": "2025-08-10T13:16:48.994102Z",
          "shell.execute_reply.started": "2025-08-10T13:16:48.988288Z",
          "shell.execute_reply": "2025-08-10T13:16:48.993069Z"
        },
        "id": "nPUaAKCqUEu4"
      },
      "outputs": [],
      "execution_count": 1
    },
    {
      "source": [
        "# IMPORTANT: SOME KAGGLE DATA SOURCES ARE PRIVATE\n",
        "# RUN THIS CELL IN ORDER TO IMPORT YOUR KAGGLE DATA SOURCES.\n",
        "import kagglehub\n",
        "kagglehub.login()\n"
      ],
      "metadata": {
        "id": "a86PQt0qUEup"
      },
      "cell_type": "code",
      "outputs": [],
      "execution_count": null
    },
    {
      "source": [
        "# IMPORTANT: RUN THIS CELL IN ORDER TO IMPORT YOUR KAGGLE DATA SOURCES,\n",
        "# THEN FEEL FREE TO DELETE THIS CELL.\n",
        "# NOTE: THIS NOTEBOOK ENVIRONMENT DIFFERS FROM KAGGLE'S PYTHON\n",
        "# ENVIRONMENT SO THERE MAY BE MISSING LIBRARIES USED BY YOUR\n",
        "# NOTEBOOK.\n",
        "\n",
        "arc_prize_2025_path = kagglehub.competition_download('arc-prize-2025')\n",
        "\n",
        "print('Data source import complete.')\n"
      ],
      "metadata": {
        "id": "lxucIuxaUEuu"
      },
      "cell_type": "code",
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# This Python 3 environment comes with many helpful analytics libraries installed\n",
        "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
        "# For example, here's several helpful packages to load\n",
        "import json\n",
        "from pathlib import Path\n",
        "\n",
        "DATA_DIR = '/kaggle/input/arc-prize-2025/'\n",
        "data = {}\n",
        "with open(Path(DATA_DIR) / 'arc-agi_training_challenges.json') as f:\n",
        "    train_challenges = json.load(f)\n",
        "with open(Path(DATA_DIR) / 'arc-agi_training_solutions.json') as f:\n",
        "    train_solutions = json.load(f)\n",
        "\n",
        "with open(Path(DATA_DIR) / 'arc-agi_evaluation_challenges.json') as f:\n",
        "    eval_challenges = json.load(f)\n",
        "with open(Path(DATA_DIR) / 'arc-agi_evaluation_solutions.json') as f:\n",
        "    eval_solutions = json.load(f)\n",
        "\n",
        "with open(Path(DATA_DIR) / 'arc-agi_test_challenges.json') as f:\n",
        "    test_challenges = json.load(f)\n",
        "\n",
        "print(f\"Training tasks: {len(train_challenges)}\")\n",
        "print(f\"Training solutions: {len(train_solutions)}\")\n",
        "print(f\"Evaluation tasks: {len(eval_challenges)}\")\n",
        "print(f\"Evaluation solutions: {len(eval_solutions)}\")\n",
        "print(f\"Test tasks: {len(test_challenges)}\")\n",
        "# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\"\n",
        "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"
      ],
      "metadata": {
        "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
        "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-08-10T13:16:42.928312Z",
          "iopub.execute_input": "2025-08-10T13:16:42.929021Z",
          "iopub.status.idle": "2025-08-10T13:16:43.447256Z",
          "shell.execute_reply.started": "2025-08-10T13:16:42.92899Z",
          "shell.execute_reply": "2025-08-10T13:16:43.446438Z"
        },
        "id": "u2cOCcecUEux",
        "outputId": "873ba645-2058-4aa7-8906-45e33a9566d6"
      },
      "outputs": [
        {
          "name": "stdout",
          "text": "Training tasks: 1000\nTraining solutions: 1000\nEvaluation tasks: 120\nEvaluation solutions: 120\nTest tasks: 240\n",
          "output_type": "stream"
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# load data\n",
        "import pandas as pd\n",
        "for i in range(0, 1):\n",
        "    key = list(train_challenges.keys())[i]\n",
        "    print(train_challenges[key])\n",
        "    print(\"----train----\")\n",
        "    print(train_solutions[key])\n",
        "    print(\"----train----\")"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-08-10T13:16:46.608872Z",
          "iopub.execute_input": "2025-08-10T13:16:46.609197Z",
          "iopub.status.idle": "2025-08-10T13:16:46.615005Z",
          "shell.execute_reply.started": "2025-08-10T13:16:46.609172Z",
          "shell.execute_reply": "2025-08-10T13:16:46.613812Z"
        },
        "id": "lbyOmN5-UEuz",
        "outputId": "039dfec9-1278-4763-ae93-9d3e33eade0e"
      },
      "outputs": [
        {
          "name": "stdout",
          "text": "{'train': [{'input': [[7, 9], [4, 3]], 'output': [[7, 9, 7, 9, 7, 9], [4, 3, 4, 3, 4, 3], [9, 7, 9, 7, 9, 7], [3, 4, 3, 4, 3, 4], [7, 9, 7, 9, 7, 9], [4, 3, 4, 3, 4, 3]]}, {'input': [[8, 6], [6, 4]], 'output': [[8, 6, 8, 6, 8, 6], [6, 4, 6, 4, 6, 4], [6, 8, 6, 8, 6, 8], [4, 6, 4, 6, 4, 6], [8, 6, 8, 6, 8, 6], [6, 4, 6, 4, 6, 4]]}], 'test': [{'input': [[3, 2], [7, 8]]}]}\n----train----\n[[[3, 2, 3, 2, 3, 2], [7, 8, 7, 8, 7, 8], [2, 3, 2, 3, 2, 3], [8, 7, 8, 7, 8, 7], [3, 2, 3, 2, 3, 2], [7, 8, 7, 8, 7, 8]]]\n----train----\n",
          "output_type": "stream"
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Import libs and utils"
      ],
      "metadata": {
        "id": "cjo9RVwpUEu1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#\n",
        "class ARCData(Dataset):\n",
        "    def __init__(self, dataframe):\n",
        "        self.task_name = dataframe[\"task_name\"]\n",
        "        self.input_values = dataframe[\"task_input\"]\n",
        "        self.task_output = databframe[\"task_output\"]\n",
        "        self.labels = dataframe[\"labels\"]\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.text)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        # train_challenges\n",
        "        df = (\n",
        "            pd.DataFrame(x)\n",
        "            .transpose()\n",
        "            .reset_index()\n",
        "            .rename(columns={'index': 'task_id'})\n",
        "            .explode('train')\n",
        "            .reset_index(drop=True)\n",
        "        )\n",
        "        df_train = df.join(pd.json_normalize(df['train']))\n",
        "        df_test = df.join(pd.json_normalize(df['test']))\n",
        "        df_train = df_train.drop(columns=\"train\")\n",
        "        df_test = df_test.drop(columns=\"test\")\n",
        "        labels = self.labels.iloc[idx]"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-08-10T07:45:34.130189Z",
          "iopub.execute_input": "2025-08-10T07:45:34.130738Z",
          "iopub.status.idle": "2025-08-10T07:45:34.140088Z",
          "shell.execute_reply.started": "2025-08-10T07:45:34.130708Z",
          "shell.execute_reply": "2025-08-10T07:45:34.138257Z"
        },
        "id": "A71hhNhCUEu5"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# custom ConvBlock\n",
        "class ParallelConvBlock(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels):\n",
        "        super(ParallelConvBlock, self).__init__()\n",
        "\n",
        "        self.conv1x3_1 = nn.Conv2d(in_channels, out_channels, kernel_size=(3, 1), padding=1)\n",
        "        self.conv1x3_2 = nn.Conv2d(out_channels, out_channels, kernel_size=(3, 1), padding=1)\n",
        "        self.conv3x1_1 = nn.Conv2d(in_channels, out_channels, kernel_size=(1, 3), padding=1)\n",
        "        self.conv3x1_2 = nn.Conv2d(out_channels, out_channels, kernel_size=(1, 3), padding=1)\n",
        "\n",
        "        self.pool(x).view(-1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        horizontal = self.conv1x3_1(x)\n",
        "        horizontal = self.conv1x3_2(horizontal)\n",
        "\n",
        "        vertical = self.conv3x1_1(x)\n",
        "        vertical = self.conv3x1_2(vertical)\n",
        "\n",
        "        out = torch.cat([horizontal, vertical], dim=1)\n",
        "        out = self.bn(out)\n",
        "        return out"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-08-10T07:44:57.411441Z",
          "iopub.status.idle": "2025-08-10T07:44:57.411959Z",
          "shell.execute_reply.started": "2025-08-10T07:44:57.411717Z",
          "shell.execute_reply": "2025-08-10T07:44:57.411741Z"
        },
        "id": "ENPbRb39UEu6"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Discriminator"
      ],
      "metadata": {
        "id": "M5QdhjUaUEu7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## data"
      ],
      "metadata": {
        "id": "M9TWwSKUUEu8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "## prepare train data\n",
        "inputs = []\n",
        "labels = []\n",
        "discriminator_train_data = []\n",
        "discriminator_test_data = []\n",
        "# get the inputs; data is a list of [inputs, labels]\n",
        "for task_name, task_data in train_challenges.items():\n",
        "    for temp in task_data[\"train\"]:\n",
        "        for k, v in temp.items():\n",
        "            # i_input = torch.nn.functional.normalize(torch.tensor([v]).float())\n",
        "            i_input = torch.tensor([v]).float()/10\n",
        "            i_label = torch.tensor([int(k == \"output\")])\n",
        "            discriminator_train_data.append((i_input, i_label, task_name))\n",
        "            # discriminator_train_data.append(i_label)\n",
        "    for temp in task_data[\"test\"]:\n",
        "        for k, v in temp.items():\n",
        "            i_input = torch.tensor([v])\n",
        "            i_label = torch.tensor([int(k == \"output\")])\n",
        "            discriminator_test_data.append(())\n",
        "print(len(discriminator_train_data))\n",
        "discriminator_train_data[0]\n",
        "\n"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-08-10T13:17:26.414106Z",
          "iopub.execute_input": "2025-08-10T13:17:26.415074Z",
          "iopub.status.idle": "2025-08-10T13:17:26.914537Z",
          "shell.execute_reply.started": "2025-08-10T13:17:26.415045Z",
          "shell.execute_reply": "2025-08-10T13:17:26.913721Z"
        },
        "id": "BatarSg-UEu9",
        "outputId": "874e9aa3-a55b-4d60-cfa6-1fe24b467313"
      },
      "outputs": [
        {
          "name": "stdout",
          "text": "6464\n",
          "output_type": "stream"
        },
        {
          "execution_count": 13,
          "output_type": "execute_result",
          "data": {
            "text/plain": "(tensor([[[0.7000, 0.9000],\n          [0.4000, 0.3000]]]),\n tensor([0]),\n '00576224')"
          },
          "metadata": {}
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "## model"
      ],
      "metadata": {
        "id": "o4O7r7mvUEu9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# create a pretrain model for discriminator\n",
        "class Discriminator(nn.Module):\n",
        "    \"\"\"\n",
        "        Discriminate if an image is a task input or a task output\n",
        "        Must be fast train for each task\n",
        "        NO LABEL\n",
        "    \"\"\"\n",
        "    def __init__(self, img_channels):\n",
        "        super(Discriminator, self).__init__()\n",
        "        # self.block1 = ParallelConvBlock(img_channels, 32)\n",
        "        # self.block2 = ParallelConvBlock(32, 64)\n",
        "\n",
        "        self.cnn_block_s = nn.Sequential(\n",
        "            nn.Conv2d(img_channels, 128, kernel_size=(3, 3), padding=1),\n",
        "            nn.Conv2d(128, 256, kernel_size=(3, 3), padding=1),\n",
        "            nn.BatchNorm2d(256),\n",
        "            nn.AdaptiveAvgPool2d((1))\n",
        "        )\n",
        "        self.cnn_block_w = nn.Sequential(\n",
        "            nn.Conv2d(img_channels, 128, kernel_size=(1, 3), padding=1),\n",
        "            nn.Conv2d(128, 256, kernel_size=(3, 3), padding=1),\n",
        "            nn.BatchNorm2d(256),\n",
        "            nn.AdaptiveAvgPool2d((1))\n",
        "\n",
        "        )\n",
        "        self.cnn_block_h = nn.Sequential(\n",
        "            nn.Conv2d(img_channels, 128, kernel_size=(3, 1), padding=1),\n",
        "            nn.Conv2d(128, 256, kernel_size=(3, 3), padding=1),\n",
        "            nn.BatchNorm2d(256),\n",
        "            nn.AdaptiveAvgPool2d((1))\n",
        "        )\n",
        "\n",
        "        self.mh_attn = nn.MultiheadAttention(256, 64, dropout=0, add_zero_attn=True)\n",
        "        self.mh_attn_norm = nn.LayerNorm(256)\n",
        "\n",
        "\n",
        "        self.ffn_block = nn.Sequential(\n",
        "            nn.Linear(256, 512, bias=False),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(512, 256, bias=False),\n",
        "            nn.LayerNorm(256),\n",
        "        )\n",
        "\n",
        "        self.fc = nn.Linear(256, 2)\n",
        "        self.softmax = nn.Softmax()\n",
        "\n",
        "\n",
        "    def forward(self, x):\n",
        "        # x = self.block1(x)\n",
        "        # # x = self.batch_norm(x)\n",
        "        # x = self.block2(x)\n",
        "        # x = self.pool(x).view(1, 128)\n",
        "        # x = self.fc(x)\n",
        "        # x = self.fc2(x)\n",
        "        # x = self.fc3(x)\n",
        "        x = x.unsqueeze(0)\n",
        "        x_s = self.cnn_block_s(x).squeeze(2, 3)\n",
        "        x_h = self.cnn_block_h(x).squeeze(2, 3)\n",
        "        x_v = self.cnn_block_w(x).squeeze(2, 3)\n",
        "        x, _ = self.mh_attn(x_s, x_h, x_v)\n",
        "        x = self.mh_attn_norm(x)\n",
        "        x = self.ffn_block(x)\n",
        "        x = self.fc(x)\n",
        "        return self.softmax(x)\n",
        "\n",
        "discriminator = Discriminator(1)\n",
        "try:\n",
        "    # discriminator.load_state_dict(torch.load(f'/kaggle/working/{discriminator.__class__.__name__}.pth', weights_only=True))\n",
        "    discriminator.load_state_dict(torch.load(f'/kaggle/working/discriminator.pth', weights_only=True))\n",
        "except FileNotFoundError as e:\n",
        "    display(e)\n",
        "except Exception as e:\n",
        "    display(e)\n",
        "\n",
        "discriminator.eval()\n"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-08-10T13:23:49.528626Z",
          "iopub.execute_input": "2025-08-10T13:23:49.529441Z",
          "iopub.status.idle": "2025-08-10T13:23:49.578863Z",
          "shell.execute_reply.started": "2025-08-10T13:23:49.529413Z",
          "shell.execute_reply": "2025-08-10T13:23:49.578193Z"
        },
        "id": "grg1Mg8zUEu-",
        "outputId": "efc13092-0e32-47b4-8689-885ab101f251"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "RuntimeError('Error(s) in loading state_dict for Discriminator:\\n\\tMissing key(s) in state_dict: \"mh_attn.in_proj_weight\", \"mh_attn.in_proj_bias\", \"mh_attn.out_proj.weight\", \"mh_attn.out_proj.bias\", \"mh_attn_norm.weight\", \"mh_attn_norm.bias\". \\n\\tUnexpected key(s) in state_dict: \"mh_attn_block.0.in_proj_weight\", \"mh_attn_block.0.in_proj_bias\", \"mh_attn_block.0.out_proj.weight\", \"mh_attn_block.0.out_proj.bias\", \"mh_attn_block.1.weight\", \"mh_attn_block.1.bias\", \"ffn_block.0.bias\", \"ffn_block.2.bias\". ')"
          },
          "metadata": {}
        },
        {
          "execution_count": 16,
          "output_type": "execute_result",
          "data": {
            "text/plain": "Discriminator(\n  (cnn_block_1): Sequential(\n    (0): Conv2d(1, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (1): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    (3): AdaptiveAvgPool2d(output_size=1)\n  )\n  (cnn_block_w): Sequential(\n    (0): Conv2d(1, 128, kernel_size=(1, 3), stride=(1, 1), padding=(1, 1))\n    (1): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    (3): AdaptiveAvgPool2d(output_size=1)\n  )\n  (cnn_block_h): Sequential(\n    (0): Conv2d(1, 128, kernel_size=(3, 1), stride=(1, 1), padding=(1, 1))\n    (1): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    (3): AdaptiveAvgPool2d(output_size=1)\n  )\n  (mh_attn): MultiheadAttention(\n    (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)\n  )\n  (mh_attn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n  (ffn_block): Sequential(\n    (0): Linear(in_features=256, out_features=512, bias=False)\n    (1): ReLU()\n    (2): Linear(in_features=512, out_features=256, bias=False)\n    (3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n  )\n  (fc): Linear(in_features=256, out_features=2, bias=True)\n  (softmax): Softmax(dim=None)\n)"
          },
          "metadata": {}
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "## train"
      ],
      "metadata": {
        "id": "6fQ3BCziUEu_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.optim as optim\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(discriminator.parameters(), lr=0.0001)\n",
        "def train_discriminator(epoch=1, batch_size = 500):\n",
        "    for epoch in range(epoch):\n",
        "        running_loss = 0.0\n",
        "        print(\"epoch\", epoch+1)\n",
        "        i = 0\n",
        "        for i_input, i_label, task_name in discriminator_train_data:\n",
        "            for sub_epoch in range(50):\n",
        "                # zero the parameter gradients\n",
        "                optimizer.zero_grad()\n",
        "                # get output\n",
        "                outputs = discriminator(i_input)\n",
        "                # calculate loss\n",
        "                loss = criterion(outputs, i_label)\n",
        "                loss.backward()\n",
        "\n",
        "                optimizer.step()\n",
        "\n",
        "                # print statistics\n",
        "                running_loss += loss.item()\n",
        "                i += 1\n",
        "\n",
        "                if (i % batch_size) == (batch_size - 1):    # print every 2000 mini-batches\n",
        "                    print(outputs, i_label)\n",
        "                    print(f'[{epoch + 1}, {i + 1:5d}] loss: {running_loss / batch_size:.3f}')\n",
        "                    running_loss = 0.0\n",
        "        torch.save(discriminator.state_dict(), f'/kaggle/working/{discriminator.__class__.__name__}.pth')\n",
        "\n",
        "train_discriminator()\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "trusted": true,
        "scrolled": true,
        "execution": {
          "iopub.status.busy": "2025-08-10T14:03:19.710564Z",
          "iopub.execute_input": "2025-08-10T14:03:19.710879Z",
          "iopub.status.idle": "2025-08-10T14:08:39.867578Z",
          "shell.execute_reply.started": "2025-08-10T14:03:19.710855Z",
          "shell.execute_reply": "2025-08-10T14:08:39.866584Z"
        },
        "id": "ATY7RSSpUEu_",
        "outputId": "1e0f408b-b035-4479-8b45-0006e7578457"
      },
      "outputs": [
        {
          "name": "stdout",
          "text": "epoch 1\ntensor([[0.3843, 0.6157]], grad_fn=<SoftmaxBackward0>) tensor([1])\n[1,   500] loss: 0.668\ntensor([[0.4226, 0.5774]], grad_fn=<SoftmaxBackward0>) tensor([1])\n[1,  1000] loss: 0.700\ntensor([[0.4544, 0.5456]], grad_fn=<SoftmaxBackward0>) tensor([1])\n[1,  1500] loss: 0.721\ntensor([[0.4653, 0.5347]], grad_fn=<SoftmaxBackward0>) tensor([1])\n[1,  2000] loss: 0.714\ntensor([[0.4707, 0.5293]], grad_fn=<SoftmaxBackward0>) tensor([1])\n[1,  2500] loss: 0.711\ntensor([[0.4736, 0.5264]], grad_fn=<SoftmaxBackward0>) tensor([1])\n[1,  3000] loss: 0.709\ntensor([[0.4726, 0.5274]], grad_fn=<SoftmaxBackward0>) tensor([1])\n[1,  3500] loss: 0.707\ntensor([[0.7141, 0.2859]], grad_fn=<SoftmaxBackward0>) tensor([1])\n[1,  4000] loss: 0.638\ntensor([[0.4907, 0.5093]], grad_fn=<SoftmaxBackward0>) tensor([1])\n[1,  4500] loss: 0.715\ntensor([[0.3166, 0.6834]], grad_fn=<SoftmaxBackward0>) tensor([1])\n[1,  5000] loss: 0.687\ntensor([[0.4592, 0.5408]], grad_fn=<SoftmaxBackward0>) tensor([1])\n[1,  5500] loss: 0.766\ntensor([[0.4298, 0.5702]], grad_fn=<SoftmaxBackward0>) tensor([1])\n[1,  6000] loss: 0.716\ntensor([[0.4640, 0.5360]], grad_fn=<SoftmaxBackward0>) tensor([1])\n[1,  6500] loss: 0.708\ntensor([[0.5234, 0.4766]], grad_fn=<SoftmaxBackward0>) tensor([1])\n[1,  7000] loss: 0.681\ntensor([[0.2339, 0.7661]], grad_fn=<SoftmaxBackward0>) tensor([1])\n[1,  7500] loss: 0.576\ntensor([[0.4046, 0.5954]], grad_fn=<SoftmaxBackward0>) tensor([1])\n[1,  8000] loss: 0.736\ntensor([[0.4538, 0.5462]], grad_fn=<SoftmaxBackward0>) tensor([1])\n[1,  8500] loss: 0.710\ntensor([[0.4891, 0.5109]], grad_fn=<SoftmaxBackward0>) tensor([1])\n[1,  9000] loss: 0.739\ntensor([[0.4612, 0.5388]], grad_fn=<SoftmaxBackward0>) tensor([1])\n[1,  9500] loss: 0.695\ntensor([[0.4505, 0.5495]], grad_fn=<SoftmaxBackward0>) tensor([1])\n[1, 10000] loss: 0.713\ntensor([[0.4696, 0.5304]], grad_fn=<SoftmaxBackward0>) tensor([1])\n[1, 10500] loss: 0.703\ntensor([[0.4785, 0.5215]], grad_fn=<SoftmaxBackward0>) tensor([1])\n[1, 11000] loss: 0.700\ntensor([[0.4792, 0.5208]], grad_fn=<SoftmaxBackward0>) tensor([1])\n[1, 11500] loss: 0.698\ntensor([[0.4271, 0.5729]], grad_fn=<SoftmaxBackward0>) tensor([1])\n[1, 12000] loss: 0.696\ntensor([[0.7802, 0.2198]], grad_fn=<SoftmaxBackward0>) tensor([1])\n[1, 12500] loss: 0.695\ntensor([[0.0315, 0.9685]], grad_fn=<SoftmaxBackward0>) tensor([1])\n[1, 13000] loss: 0.627\ntensor([[0.6583, 0.3417]], grad_fn=<SoftmaxBackward0>) tensor([1])\n[1, 13500] loss: 0.828\ntensor([[0.0145, 0.9855]], grad_fn=<SoftmaxBackward0>) tensor([1])\n[1, 14000] loss: 0.462\ntensor([[0.0057, 0.9943]], grad_fn=<SoftmaxBackward0>) tensor([1])\n[1, 14500] loss: 0.416\n",
          "output_type": "stream"
        },
        {
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipykernel_31/3477969101.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     30\u001b[0m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdiscriminator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34mf'/kaggle/working/{discriminator.__class__.__name__}.pth'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 32\u001b[0;31m \u001b[0mtrain_discriminator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     33\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipykernel_31/3477969101.py\u001b[0m in \u001b[0;36mtrain_discriminator\u001b[0;34m(epoch, batch_size)\u001b[0m\n\u001b[1;32m     18\u001b[0m                 \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m                 \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m                 \u001b[0;31m# print statistics\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/optim/optimizer.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    485\u001b[0m                             )\n\u001b[1;32m    486\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 487\u001b[0;31m                 \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    488\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_optimizer_step_code\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    489\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/optim/optimizer.py\u001b[0m in \u001b[0;36m_use_grad\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     89\u001b[0m             \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_grad_enabled\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdefaults\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"differentiable\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     90\u001b[0m             \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dynamo\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgraph_break\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 91\u001b[0;31m             \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     92\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m             \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dynamo\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgraph_break\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/optim/adam.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, closure)\u001b[0m\n\u001b[1;32m    221\u001b[0m             )\n\u001b[1;32m    222\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 223\u001b[0;31m             adam(\n\u001b[0m\u001b[1;32m    224\u001b[0m                 \u001b[0mparams_with_grad\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    225\u001b[0m                 \u001b[0mgrads\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/optim/optimizer.py\u001b[0m in \u001b[0;36mmaybe_fallback\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    152\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mdisabled_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    153\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 154\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    155\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    156\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mmaybe_fallback\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/optim/adam.py\u001b[0m in \u001b[0;36madam\u001b[0;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, foreach, capturable, differentiable, fused, grad_scale, found_inf, has_complex, amsgrad, beta1, beta2, lr, weight_decay, eps, maximize)\u001b[0m\n\u001b[1;32m    782\u001b[0m         \u001b[0mfunc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_single_tensor_adam\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    783\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 784\u001b[0;31m     func(\n\u001b[0m\u001b[1;32m    785\u001b[0m         \u001b[0mparams\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    786\u001b[0m         \u001b[0mgrads\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/optim/adam.py\u001b[0m in \u001b[0;36m_single_tensor_adam\u001b[0;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, grad_scale, found_inf, amsgrad, has_complex, beta1, beta2, lr, weight_decay, eps, maximize, capturable, differentiable)\u001b[0m\n\u001b[1;32m    430\u001b[0m                 \u001b[0mdenom\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mexp_avg_sq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mbias_correction2_sqrt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0meps\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    431\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 432\u001b[0;31m             \u001b[0mparam\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maddcdiv_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexp_avg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdenom\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mstep_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    433\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    434\u001b[0m         \u001b[0;31m# Lastly, switch back to complex view\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ],
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error"
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# real training\n",
        "train_discriminator(50)"
      ],
      "metadata": {
        "trusted": true,
        "id": "we9OPMRzUEvA"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Operator"
      ],
      "metadata": {
        "id": "LAn81pgcUEvA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Data for OperatorEncoder"
      ],
      "metadata": {
        "id": "8RzV0rdkUEvA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# testing\n",
        "random_size = tuple(torch.randint(2, 10, (2,)))\n",
        "print(torch.randint(\n",
        "    1, 10,\n",
        "    random_size\n",
        "    )\n",
        ")\n",
        "print(\"-----\")\n",
        "print(torch.randint(\n",
        "    1, 10,\n",
        "    random_size\n",
        "    )[:, :random_size[1] - 1]\n",
        ")\n"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-08-09T14:54:39.418762Z",
          "iopub.execute_input": "2025-08-09T14:54:39.419126Z",
          "iopub.status.idle": "2025-08-09T14:54:39.435373Z",
          "shell.execute_reply.started": "2025-08-09T14:54:39.419103Z",
          "shell.execute_reply": "2025-08-09T14:54:39.434121Z"
        },
        "id": "stt162SwUEvB",
        "outputId": "ede96550-e64b-4326-b4ef-438770161801"
      },
      "outputs": [
        {
          "name": "stdout",
          "text": "tensor([[6, 7, 8, 7, 7, 1, 9],\n        [1, 8, 8, 7, 5, 5, 3],\n        [3, 8, 3, 2, 2, 1, 4],\n        [6, 2, 1, 1, 7, 1, 8],\n        [3, 7, 9, 7, 4, 7, 8],\n        [5, 6, 4, 1, 8, 8, 7]])\n-----\ntensor([[6, 1, 1, 7, 5, 1],\n        [1, 8, 8, 4, 9, 8],\n        [1, 7, 7, 7, 5, 8],\n        [2, 1, 2, 1, 9, 5],\n        [8, 3, 7, 9, 2, 9],\n        [9, 3, 9, 3, 1, 6]])\nOriginal tensor:\n tensor([[1, 2],\n        [3, 4]])\nTensor after adding 1:\n tensor([[2, 3],\n        [4, 5]])\n",
          "output_type": "stream"
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# create data for training OperatorEncoder\n",
        "class OperatorEncoderDataset(Dataset):\n",
        "    def __init__(self, sample_size=100):\n",
        "        # operator learning\n",
        "        self.op_label = []\n",
        "        self.op_input = []\n",
        "        self.op_output = []\n",
        "        # w_add\n",
        "        for i in range(0, sample_size):\n",
        "            random_size = tuple(torch.randint(1, 10, (2,)))\n",
        "            seed = torch.randint(0, 10, random_size)\n",
        "\n",
        "            self.op_input.append(seed[:, :random_size[1] - (i % 9) + 1])\n",
        "            if (i / 10) % 2 == 0:\n",
        "                self.op_output.append(seed)\n",
        "            self.op_label.append(\"w_add\")\n",
        "        # w_substract\n",
        "        for i in range(0, sample_size):\n",
        "            \"\"\"\"\"\"\n",
        "            random_size = tuple(torch.randint(1, 10, (2,)))\n",
        "            seed = torch.randint(0, 10, random_size)\n",
        "\n",
        "            self.op_input.append(seed)\n",
        "            if (i / 10) % 2 == 0:\n",
        "                self.op_output.append(seed[:, :random_size[1] - (i % 9) + 1])\n",
        "            self.op_label.append(\"w_subtract\")\n",
        "        # h_add\n",
        "        for i, j in zip(range(0, sample_size), range(0, 100)):\n",
        "            random_size = tuple(torch.randint(1, 10, (2,)))\n",
        "            seed = torch.randint(0, 10, random_size)\n",
        "\n",
        "            self.op_input.append(seed)\n",
        "            if (i / 10) % 2 == 0:\n",
        "                self.op_output.append(seed[:random_size[1] - (i % 9) + 1, :])\n",
        "            self.op_label.append(\"h_add\")\n",
        "        # h_substract\n",
        "        for i, j in zip(range(0, sample_size), range(0, 100)):\n",
        "            random_size = tuple(torch.randint(1, 10, (2,)))\n",
        "            seed = torch.randint(0, 10, random_size)\n",
        "\n",
        "            self.op_input.append(seed[:random_size[1] - (i % 9) + 1, :] )\n",
        "            if (i / 10) % 2 == 0:\n",
        "                self.op_output.append(seed)\n",
        "            self.op_label.append(\"h_subtract\")\n",
        "        # # transform\n",
        "        # for i, j in zip(range(0, sample_size), range(0, 100)):\n",
        "        #     torch.randint(0, 10, (3,))\n",
        "        #     self.op_input.append()\n",
        "        #     self.op_output.append()\n",
        "        #     self.op_label.append(\"transform\")\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.text)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        \"\"\"\n",
        "            Get item\n",
        "        \"\"\"\n",
        "        op_input = self.op_input[idx]\n",
        "        op_output = self.op_output[idx]\n",
        "        op_label = self.op_label[idx]\n",
        "        return op_input, op_output, op_label\n",
        "\n",
        "op_dataset = OperatorEncoderDataset()"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-08-10T14:08:41.486103Z",
          "iopub.execute_input": "2025-08-10T14:08:41.486393Z",
          "iopub.status.idle": "2025-08-10T14:08:41.52792Z",
          "shell.execute_reply.started": "2025-08-10T14:08:41.486372Z",
          "shell.execute_reply": "2025-08-10T14:08:41.52684Z"
        },
        "id": "jrmG_ZhtUEvB",
        "outputId": "00f6abc6-6b13-4a89-9fc5-fb2d18d1982c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 297
        }
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "list.append() takes exactly one argument (0 given)",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-3236771095.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     62\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mop_input\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mop_output\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mop_label\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 64\u001b[0;31m \u001b[0mop_dataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mOperatorEncoderDataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/tmp/ipython-input-3236771095.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, sample_size)\u001b[0m\n\u001b[1;32m     46\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mj\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m100\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m             \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 48\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mop_input\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     49\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mop_output\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mop_label\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"transform\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: list.append() takes exactly one argument (0 given)"
          ]
        }
      ],
      "execution_count": 3
    },
    {
      "cell_type": "code",
      "source": [
        "i1_dim = 3\n",
        "i2_dim = 3\n",
        "m = nn.Bilinear(i1_dim, i2_dim, 2)\n",
        "m.weight.data.fill_(0)\n",
        "input1 = torch.randn(2, i1_dim).fill_(0)\n",
        "print(input1)\n",
        "input2 = torch.randn(2, i2_dim).fill_(1)\n",
        "print(input2)\n",
        "output = m(input1, input2)\n",
        "print(output)"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-08-09T06:58:58.330794Z",
          "iopub.execute_input": "2025-08-09T06:58:58.331127Z",
          "iopub.status.idle": "2025-08-09T06:58:58.34127Z",
          "shell.execute_reply.started": "2025-08-09T06:58:58.331103Z",
          "shell.execute_reply": "2025-08-09T06:58:58.34008Z"
        },
        "id": "fNKrVr4TUEvC",
        "outputId": "c141d5db-8bc5-4e3f-a38d-61af102c63fa"
      },
      "outputs": [
        {
          "name": "stdout",
          "text": "tensor([[0., 0., 0.],\n        [0., 0., 0.]])\ntensor([[1., 1., 1.],\n        [1., 1., 1.]])\ntensor([[0.1775, 0.1738],\n        [0.1775, 0.1738]], grad_fn=<AddBackward0>)\n",
          "output_type": "stream"
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# create a pretrain model for discriminator\n",
        "class OperatorLearningBlock(nn.Module):\n",
        "    def __init__(self, ):\n",
        "        \"\"\"\n",
        "            The inputs: task_input, concate(task_input, task_output)\n",
        "            condition = embedding(task_input)\n",
        "            rule = embedding(task_input, task_output)\n",
        "            Bilinear(condition, rule, task_output(h x w))\n",
        "        \"\"\"\n",
        "        # create a block of operator\n",
        "        # use bilinear_transform to change color\n",
        "        self.bilinear_1 = nn.Sequetial([\n",
        "            nn.Bilinear(2, 3, 15)\n",
        "        ])\n",
        "        self.op_layers = nn.ModuleList([nn.Linear(10, 10) for _ in range(num_layers)])\n",
        "        self.scalar.weight.data.fill_(0.01)\n",
        "\n",
        "    def forward(self, task_input, task_output):\n",
        "        \"\"\"\n",
        "            Bilinear\n",
        "        \"\"\"\n",
        "        self.bilinear_1(task_input)\n",
        "\n",
        "            for layer in self.layers:\n",
        "                x = layer(x)\n",
        "            return x\n",
        "\n",
        "\n",
        "class OperatorEncoder(nn.Module):\n",
        "    \"\"\"\n",
        "        Encode operator\n",
        "    \"\"\"\n",
        "    def __init__(self, discriminator:nn.Module, embed_dim=2707, num_heads=32, ff_dim=2511, dropout=0.1):\n",
        "        super(Recoginzer, self).__init__()\n",
        "        self.self_attn = nn.MultiheadAttention(embed_dim, num_heads, dropout=dropout)\n",
        "        self.ffn = nn.Sequential(\n",
        "            nn.Linear(embed_dim, ff_dim),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(ff_dim, embed_dim)\n",
        "        )\n",
        "        self.norm1 = nn.LayerNorm(embed_dim)\n",
        "        self.norm2 = nn.LayerNorm(embed_dim)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, src, src_mask=None):\n",
        "        src2 = self.self_attn(src, src, src, attn_mask=src_mask)[0]\n",
        "        src = self.norm1(src + self.dropout(src2))\n",
        "        src2 = self.ffn(src)\n",
        "        src = self.norm2(src + self.dropout(src2))\n",
        "        return src\n",
        "\n",
        "\n",
        "operator_encoder = OperatorEncoder()\n",
        "try:\n",
        "    operator_encoder.load_state_dict(torch.load(f'/kaggle/working/{OperatorEncoder.__class__.__name__}.pth', weights_only=True))\n",
        "except FileNotFoundError as e:\n",
        "    display(e)\n",
        "\n",
        "operator_encoder.eval()"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-08-08T13:14:43.713362Z",
          "iopub.execute_input": "2025-08-08T13:14:43.713705Z",
          "iopub.status.idle": "2025-08-08T13:14:43.733189Z",
          "shell.execute_reply.started": "2025-08-08T13:14:43.713675Z",
          "shell.execute_reply": "2025-08-08T13:14:43.732028Z"
        },
        "id": "djrTO6iFUEvC",
        "outputId": "29564457-9f2b-4c29-dda6-9383bac93af5"
      },
      "outputs": [
        {
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipykernel_31/4140460743.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m \u001b[0moperator_encoder\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mOperatorEncoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m     \u001b[0moperator_encoder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_state_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'/kaggle/working/{OperatorEncoder.__class__.__name__}.pth'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweights_only\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: OperatorEncoder.__init__() missing 3 required positional arguments: 'embed_dim', 'num_heads', and 'ff_dim'"
          ],
          "ename": "TypeError",
          "evalue": "OperatorEncoder.__init__() missing 3 required positional arguments: 'embed_dim', 'num_heads', and 'ff_dim'",
          "output_type": "error"
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# prepare to count predictions for each class\n",
        "labels = [\"output\", \"input\"]\n",
        "correct_pred = {classname: 0 for classname in labels}\n",
        "total_pred = {classname: 0 for classname in labels}\n",
        "\n",
        "# again no gradients needed\n",
        "with torch.no_grad():\n",
        "    for data in testloader:\n",
        "        images, labels = data\n",
        "        outputs = discriminator(images)\n",
        "        _, predictions = torch.max(outputs, 1)\n",
        "        # collect the correct predictions for each class\n",
        "        for label, prediction in zip(labels, predictions):\n",
        "            if label == prediction:\n",
        "                correct_pred[classes[label]] += 1\n",
        "            total_pred[classes[label]] += 1\n",
        "\n",
        "\n",
        "# print accuracy for each class\n",
        "for classname, correct_count in correct_pred.items():\n",
        "    accuracy = 100 * float(correct_count) / total_pred[classname]\n",
        "    print(f'Accuracy for class: {classname:5s} is {accuracy:.1f} %')"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-08-07T10:11:31.778377Z",
          "iopub.execute_input": "2025-08-07T10:11:31.778708Z",
          "iopub.status.idle": "2025-08-07T10:11:31.795044Z",
          "shell.execute_reply.started": "2025-08-07T10:11:31.778687Z",
          "shell.execute_reply": "2025-08-07T10:11:31.79404Z"
        },
        "id": "wqox0GvKUEvD",
        "outputId": "eda8a849-95f8-42d0-d23b-0cef67d4a1f7"
      },
      "outputs": [
        {
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipykernel_31/787847548.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;31m# again no gradients needed\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0mdata\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtestloader\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m         \u001b[0mimages\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdiscriminator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimages\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'testloader' is not defined"
          ],
          "ename": "NameError",
          "evalue": "name 'testloader' is not defined",
          "output_type": "error"
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "class Solver(nn.Module):\n",
        "    def __init__(self, vocab_size, d_model=512, nhead=8, num_layers=6):\n",
        "        super().__init__()\n",
        "        self.embedding = nn.Embedding(vocab_size, d_model)\n",
        "        self.transformer = nn.Transformer(d_model=d_model, nhead=nhead, num_encoder_layers=num_layers, num_decoder_layers=num_layers)\n",
        "        self.fc_out = nn.Linear(d_model, vocab_size)\n",
        "\n",
        "    def forward(self, src, tgt):\n",
        "        src = self.embedding(src).permute(1, 0, 2)  # (S, N, E)\n",
        "        tgt = self.embedding(tgt).permute(1, 0, 2)\n",
        "        output = self.transformer(src, tgt)\n",
        "        return self.fc_out(output).permute(1, 0, 2)\n",
        "\n",
        "class TransformerDecoderBlock(nn.Module):\n",
        "    def __init__(self, embed_dim, num_heads, ff_dim, dropout=0.1):\n",
        "        super(TransformerDecoderBlock, self).__init__()\n",
        "        self.self_attn = nn.MultiheadAttention(embed_dim, num_heads, dropout=dropout)\n",
        "        self.cross_attn = nn.MultiheadAttention(embed_dim, num_heads, dropout=dropout)\n",
        "        self.ffn = nn.Sequential(\n",
        "            nn.Linear(embed_dim, ff_dim),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(ff_dim, embed_dim)\n",
        "        )\n",
        "        self.norm1 = nn.LayerNorm(embed_dim)\n",
        "        self.norm2 = nn.LayerNorm(embed_dim)\n",
        "        self.norm3 = nn.LayerNorm(embed_dim)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, tgt, memory, tgt_mask=None, memory_mask=None):\n",
        "        tgt2 = self.self_attn(tgt, tgt, tgt, attn_mask=tgt_mask)[0]\n",
        "        tgt = self.norm1(tgt + self.dropout(tgt2))\n",
        "        tgt2 = self.cross_attn(tgt, memory, memory, attn_mask=memory_mask)[0]\n",
        "        tgt = self.norm2(tgt + self.dropout(tgt2))\n",
        "        tgt2 = self.ffn(tgt)\n",
        "        tgt = self.norm3(tgt + self.dropout(tgt2))\n",
        "        return tgt\n",
        "\n",
        "\n",
        "class Thinkable(nn.Module):\n",
        "    \"\"\"\n",
        "        This is the main class\n",
        "    \"\"\"\n",
        "    def __init__(self, epochs=10):\n",
        "        super().__init__()\n",
        "        self.fc1 = nn.Linear(256, 256)\n",
        "        self.pixel_allocator = nn.Bilinear(256, 256, 10)\n",
        "        self.softmax = nn.Softmax(256)\n",
        "\n",
        "    def _selector(self, x):\n",
        "        \"\"\"\n",
        "            activator\n",
        "        \"\"\"\n",
        "        return (x>0.5).float()\n",
        "\n",
        "\n",
        "    def separate_channel(self, x, color_channel):\n",
        "        \"\"\"\n",
        "            Separated image into range 0-9 channels\n",
        "        \"\"\"\n",
        "        return [float(data == target_number) for target_number in color_channel]\n",
        "\n",
        "    def allocate_pixel(self, task_input, task_output, proposed_solution, color_channel=range(0, 10)):\n",
        "        \"\"\"\n",
        "            set pixel color\n",
        "        \"\"\"\n",
        "        # pick the color\n",
        "\n",
        "        # choose the pixel position\n",
        "        pixel_allocator(task_input, task_output)\n",
        "        self.softmax\n",
        "        return\n",
        "\n",
        "    def reshape(self, task_input:torch.Tensor, task_output:torch.Tensor):\n",
        "        \"\"\"\n",
        "            predict the output shape from task examples\n",
        "        \"\"\"\n",
        "        # task input shape\n",
        "        task_input.shape\n",
        "        # task output shape\n",
        "        task_output.shape\n",
        "        # task input - task out shape translate\n",
        "        width = nn.Linear(256, 256)()\n",
        "        height = nn.Linear(256, 256)\n",
        "        return\n",
        "\n",
        "    def learn_to_solve(self, task_input, task_output):\n",
        "        \"\"\"\n",
        "\n",
        "        \"\"\"\n",
        "\n",
        "    def learn_to_know(self, model, task_input, task_output):\n",
        "        \"\"\"\n",
        "            generate the solver for task\n",
        "        \"\"\"\n",
        "        # prepare train data\n",
        "        # generalize\n",
        "\n",
        "        tgt_input = task_input\n",
        "        tgt_output = task_output\n",
        "\n",
        "        criterion = nn.CrossEntropyLoss()\n",
        "        optimizer = optim.SGD(net.parameters(), lr=0.001, momentum=0.9)\n",
        "\n",
        "        print('Finished Training')\n",
        "\n",
        "        for epoch in range(self.epochs):\n",
        "            for src, tgt in dataloader:\n",
        "                optimizer.zero_grad()\n",
        "                output = model(src, tgt_input)\n",
        "                loss = criterion(output.reshape(-1, vocab_size), tgt_output.reshape(-1))\n",
        "                loss.backward()\n",
        "                optimizer.step()\n",
        "\n",
        "    def think(self, model, task_input, task_output):\n",
        "        \"\"\"\n",
        "            train the model based on few task examples\n",
        "        \"\"\"\n",
        "        return model\n",
        "\n",
        "\n",
        "    def forward(self, task_input, task_output, target_tensor):\n",
        "        \"\"\"\n",
        "            Operator layer\n",
        "        \"\"\"\n",
        "        return solution\n",
        "\n"
      ],
      "metadata": {
        "trusted": true,
        "scrolled": true,
        "id": "Dnyq0L76UEvD"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "def train_model(model):\n",
        "    \"\"\"\n",
        "        Training ground\n",
        "    \"\"\"\n",
        "    # Prepare train data\n",
        "\n",
        "    # Train Solver\n",
        "\n",
        "    # Train Analyzer\n",
        "\n",
        "    #\n",
        "    for epoch in range(epochs):\n",
        "        for src, tgt in dataloader:\n",
        "            tgt_input = tgt[:, :-1]\n",
        "            tgt_output = tgt[:, 1:]\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            output = model(src, tgt_input)\n",
        "            loss = criterion(output.reshape(-1, vocab_size), tgt_output.reshape(-1))\n",
        "            loss.backward()\n",
        "            optimizer.step()\n"
      ],
      "metadata": {
        "trusted": true,
        "id": "e-Y7KP_JUEvE"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "data = np.random.random_integers(5, size=(3,2))\n",
        "print(data, data.shape)\n",
        "for target_number in range(0, 9):\n",
        "    print(torch.where(data == target_number*torch.ones(data.shape, -1)))"
      ],
      "metadata": {
        "trusted": true,
        "id": "JBv0nWGrUEvF"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "# GAN model"
      ],
      "metadata": {
        "id": "rv1Dg11PUEvF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "\n",
        "class Generator(nn.Module):\n",
        "    def __init__(self, noise_dim, label_dim, img_channels):\n",
        "        super(Generator, self).__init__()\n",
        "        self.label_embed = nn.Embedding(label_dim, label_dim)\n",
        "        self.fc = nn.Linear(noise_dim + label_dim, 128 * 7 * 7)\n",
        "\n",
        "        self.block1 = ParallelConvBlock(128, 64)\n",
        "        self.block2 = ParallelConvBlock(64, 32)\n",
        "        self.block3 = nn.Conv2d()\n",
        "        self.upsample = nn.Upsample(scale_factor=2)\n",
        "        self.final = nn.Conv2d(64, img_channels, kernel_size=3, padding=1)\n",
        "        self.tanh = nn.Tanh()\n",
        "\n",
        "    def forward(self, noise, labels):\n",
        "        label_embed = self.label_embed(labels)\n",
        "        x = torch.cat([noise, label_embed], dim=1)\n",
        "        x = self.fc(x).view(-1, 128, 7, 7)\n",
        "        x = self.upsample(self.block1(x))\n",
        "        x = self.upsample(self.block2(x))\n",
        "        return self.tanh(self.final(x))\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "trusted": true,
        "id": "y4IcekZjUEvF"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "trusted": true,
        "id": "NQfchkPPUEvG"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Transformer model ( next operation)"
      ],
      "metadata": {
        "id": "unaCHtBFUEvG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "\n",
        "class TransformerEncoderBlock(nn.Module):\n",
        "    def __init__(self, embed_dim, num_heads, ff_dim, dropout=0.1):\n",
        "        super(TransformerEncoderBlock, self).__init__()\n",
        "        self.self_attn = nn.MultiheadAttention(embed_dim, num_heads, dropout=dropout)\n",
        "        self.ffn = nn.Sequential(\n",
        "            nn.Linear(embed_dim, ff_dim),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(ff_dim, embed_dim)\n",
        "        )\n",
        "        self.norm1 = nn.LayerNorm(embed_dim)\n",
        "        self.norm2 = nn.LayerNorm(embed_dim)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, src, src_mask=None):\n",
        "        src2 = self.self_attn(src, src, src, attn_mask=src_mask)[0]\n",
        "        src = self.norm1(src + self.dropout(src2))\n",
        "        src2 = self.ffn(src)\n",
        "        src = self.norm2(src + self.dropout(src2))\n",
        "        return src\n",
        "\n",
        "class TransformerDecoderBlock(nn.Module):\n",
        "    def __init__(self, embed_dim, num_heads, ff_dim, dropout=0.1):\n",
        "        super(TransformerDecoderBlock, self).__init__()\n",
        "        self.self_attn = nn.MultiheadAttention(embed_dim, num_heads, dropout=dropout)\n",
        "        self.cross_attn = nn.MultiheadAttention(embed_dim, num_heads, dropout=dropout)\n",
        "        self.ffn = nn.Sequential(\n",
        "            nn.Linear(embed_dim, ff_dim),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(ff_dim, embed_dim)\n",
        "        )\n",
        "        self.norm1 = nn.LayerNorm(embed_dim)\n",
        "        self.norm2 = nn.LayerNorm(embed_dim)\n",
        "        self.norm3 = nn.LayerNorm(embed_dim)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, tgt, memory, tgt_mask=None, memory_mask=None):\n",
        "        tgt2 = self.self_attn(tgt, tgt, tgt, attn_mask=tgt_mask)[0]\n",
        "        tgt = self.norm1(tgt + self.dropout(tgt2))\n",
        "        tgt2 = self.cross_attn(tgt, memory, memory, attn_mask=memory_mask)[0]\n",
        "        tgt = self.norm2(tgt + self.dropout(tgt2))\n",
        "        tgt2 = self.ffn(tgt)\n",
        "        tgt = self.norm3(tgt + self.dropout(tgt2))\n",
        "        return tgt\n",
        "\n"
      ],
      "metadata": {
        "trusted": true,
        "id": "D9z1XOw9UEvI"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "## loss function"
      ],
      "metadata": {
        "id": "qAukSRmuUEvJ"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "trusted": true,
        "id": "5fxG0BzoUEvK"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Train model"
      ],
      "metadata": {
        "id": "x3Vp_Xz1UEvK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "# Train your model\n",
        "model = Thinkable()\n",
        "torch.save(model.state_dict(), '/kaggle/working/my_model.pth')\n"
      ],
      "metadata": {
        "trusted": true,
        "id": "aCW7XLWnUEvK"
      },
      "outputs": [],
      "execution_count": null
    }
  ]
}