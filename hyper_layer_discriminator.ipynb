{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":91496,"databundleVersionId":11802066,"sourceType":"competition"}],"dockerImageVersionId":31012,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Import libs and utils","metadata":{}},{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\nimport json\nfrom pathlib import Path\n\nDATA_DIR = '/kaggle/input/arc-prize-2025/'\ndata = {}\nwith open(Path(DATA_DIR) / 'arc-agi_training_challenges.json') as f:\n    train_challenges = json.load(f)\nwith open(Path(DATA_DIR) / 'arc-agi_training_solutions.json') as f:\n    train_solutions = json.load(f)\n\nwith open(Path(DATA_DIR) / 'arc-agi_evaluation_challenges.json') as f:\n    eval_challenges = json.load(f)\nwith open(Path(DATA_DIR) / 'arc-agi_evaluation_solutions.json') as f:\n    eval_solutions = json.load(f)\n\nwith open(Path(DATA_DIR) / 'arc-agi_test_challenges.json') as f:\n    test_challenges = json.load(f)\n\nprint(f\"Training tasks: {len(train_challenges)}\")\nprint(f\"Training solutions: {len(train_solutions)}\")\nprint(f\"Evaluation tasks: {len(eval_challenges)}\")\nprint(f\"Evaluation solutions: {len(eval_solutions)}\")\nprint(f\"Test tasks: {len(test_challenges)}\")\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session# Utils\nimport random\nimport matplotlib.pyplot as plt\nimport torch\nimport numpy as np\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.utils.data.dataset import Dataset\nfrom torch.utils.data import DataLoader\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nfrom torchsummary import summary\n\ndef start_train(train_func, *args, **kwargs):\n    for epoch in range(50):\n        running_loss = 0.0\n        print(\"epoch\", epoch+1)\n        i = 0\n        train_func(*args, **kwargs)\n# load data\nimport pandas as pd\nfor i in range(0, 1):\n    key = list(train_challenges.keys())[i]\n    print(train_challenges[key])\n    print(\"----train----\")\n    print(train_solutions[key])\n    print(\"----train----\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-19T02:07:56.853508Z","iopub.execute_input":"2025-08-19T02:07:56.854092Z","iopub.status.idle":"2025-08-19T02:07:57.544097Z","shell.execute_reply.started":"2025-08-19T02:07:56.854063Z","shell.execute_reply":"2025-08-19T02:07:57.543243Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Discriminator","metadata":{}},{"cell_type":"markdown","source":"## data_1: data from ARC","metadata":{}},{"cell_type":"code","source":"## prepare train data\ninputs = []\nlabels = []\ndiscriminator_train_data = []\ndiscriminator_test_data = []\n# get the inputs; data is a list of [inputs, labels]\nfor task_name, task_data in train_challenges.items():\n    task_big_bag = [task_name, []]\n    task_bag = [[], []]\n    for temp in task_data[\"train\"]:\n        task_bag[0].append(temp[\"input\"])\n        task_bag[1].append(temp[\"output\"])\n    for ii in task_bag[0]:\n        temp = [task_bag[0], task_bag[1], ii, 0]\n        task_big_bag[1].append(temp)\n    for ii in task_bag[1]:\n        temp = [task_bag[0], task_bag[1], ii, 1]\n        task_big_bag[1].append(temp)\n\n    discriminator_train_data.append(task_big_bag)\nprint(len(discriminator_train_data))\nprint(len(discriminator_train_data[0]))\nprint(len(discriminator_train_data[0][1]))\nlist(train_challenges.items())[0][1][\"train\"]\n# train using permutation (asume permutation will preserve correlation)\n\n        ","metadata":{"trusted":true,"scrolled":true,"execution":{"iopub.status.busy":"2025-08-19T02:07:57.545715Z","iopub.execute_input":"2025-08-19T02:07:57.546082Z","iopub.status.idle":"2025-08-19T02:07:57.571568Z","shell.execute_reply.started":"2025-08-19T02:07:57.546059Z","shell.execute_reply":"2025-08-19T02:07:57.570689Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"test = [[[1, 2], [3, 4]]]\ntest_1 = torch.stack([torch.stack([torch.Tensor(t) == i for i in range(0, 10)]).float() for t in test]) \nprint(test_1.shape)\ntest_2 = [torch.roll(torch.Tensor(t), i, 1) for i in range(0, 10) for t in [torch.stack([torch.Tensor(t) == i for i in range(0, 10)]).float() for t in test]]\ntest_2 = torch.stack([torch.roll(torch.Tensor(t), i, 1) for i in range(0, 10) for t in [torch.stack([torch.Tensor(t) == i for i in range(0, 10)]).float() for t in test]])\nprint(test_2.shape)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-19T02:07:57.572699Z","iopub.execute_input":"2025-08-19T02:07:57.572954Z","iopub.status.idle":"2025-08-19T02:07:57.588668Z","shell.execute_reply.started":"2025-08-19T02:07:57.572935Z","shell.execute_reply":"2025-08-19T02:07:57.587697Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import torch\nt1 = torch.ones((3, 4)).squeeze()\nt2 = torch.zeros((3, 4)).squeeze()\nt3 = torch.stack([t1, t2], dim=1)\nprint(t3.shape)\nt4 = t3.view(1, t3.shape[1]*t3.shape[0], -1)\nprint(t4)\n\nt5 = nn.Conv2d(1, 256, kernel_size=(2, 1), stride=(2, 1))(t4)\nprint(t5.shape)\nt6 = nn.AdaptiveAvgPool2d((1))(t5)\nprint(t6.shape)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-19T05:06:34.646530Z","iopub.execute_input":"2025-08-19T05:06:34.647217Z","iopub.status.idle":"2025-08-19T05:06:34.657484Z","shell.execute_reply.started":"2025-08-19T05:06:34.647192Z","shell.execute_reply":"2025-08-19T05:06:34.656546Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## model","metadata":{}},{"cell_type":"code","source":"# create a pretrain model for discriminator\n# do 2 things: learn how to pick up pattern from inputs and outputs\n# \nimport torch\nimport torch.nn as nn\n\nclass PositionalEncoding(nn.Module):\n    def __init__(self, d_model: int, dropout: float = 0.1, max_len: int = 5000):\n        super().__init__()\n        self.dropout = nn.Dropout(p=dropout)\n\n        position = torch.arange(max_len).unsqueeze(1)\n        div_term = torch.exp(torch.arange(0, d_model, 2) * (-math.log(10000.0) / d_model))\n        pe = torch.zeros(max_len, 1, d_model)\n        pe[:, 0, 0::2] = torch.sin(position * div_term)\n        pe[:, 0, 1::2] = torch.cos(position * div_term)\n        self.register_buffer('pe', pe)\n\n    def forward(self, x: torch.Tensor) -> torch.Tensor:\n        \"\"\"\n        Arguments:\n            x: Tensor, shape ``[seq_len, batch_size, embedding_dim]``\n        \"\"\"\n        x = x + self.pe[:x.size(0)]\n        return self.dropout(x)\n\n\nclass DynamicLinear(nn.Linear):\n    def __init__(self, weights=None, bias=None, trainable=True, *arg, **kwargs):\n        super().__init__(*arg, **kwargs)\n        with torch.no_grad():\n            self.weight.copy_(weights)\n            self.bias.copy_(bias)\n        if not trainable:\n            for param in self.parameters():\n                param.requires_grad = False\n\n# ---------------------------------------- #   \nclass BatchMatMulLayer(nn.Module):\n    def __init__(self):\n        super().__init__()\n\n    def forward(self, x, weight, bias):\n        # x: (batch, in_dim)\n        # weight: (batch, out_dim, in_dim)\n        # bias: (batch, out_dim)\n        out = torch.bmm(weight, x.unsqueeze(-1)).squeeze(-1) + bias\n        return out\n\n# ---------------------------------------- #\nclass HyperEncoder(nn.Module):\n    def __init__(self, in_features, out_features, weight, bias, filters:torch.Tensor):\n        super().__init__()\n        \n        # CNN\n       \n        filters = filters\n        F.conv2d(inputs, filters, padding=1)\n\n        \n        # Define learnable weights and bias as nn.Parameter\n        self.weight = nn.Parameter(torch.randn(out_features, in_features))\n        self.bias = nn.Parameter(torch.randn(out_features))\n\n        # Initialize parameters (optional, but good practice)\n        nn.init.kaiming_uniform_(self.weight, nonlinearity='relu')\n        nn.init.zeros_(self.bias)\n\n    def forward(self, x):\n        # Perform matrix multiplication and add bias\n        x = torch.matmul(x, self.weight.T) + self.bias\n        # Apply ReLU activation\n        x = torch.relu(x)\n        return x\n\n# ---------------------------------------- #\nclass DynamicLinearGenerator(nn.Module):\n    def __init__(self, input_dim, params_dim, target_in_dim, target_out_dim):\n        super().__init__()\n        self.input_dim = input_dim\n        self.target_in_dim = target_in_dim\n        self.target_out_dim = target_out_dim\n\n        self.filter_embedding = nn.Embedding()\n\n        self.\n        # self.multihead_attention = nn.MultiheadAttention(Q, K, V)\n        \n        self.ffn_block = nn.Sequential(\n            nn.Linear(input_dim, params_dim, bias=True),\n            nn.ReLU(),\n            nn.Linear(params_dim, target_in_dim * target_out_dim + target_out_dim, bias=True),\n        )\n        \n        \n        # Q,K,V will be Q = Task Input, K = Task Output, V = F(Input, Output)\n\n    \n    def forward(self, cross_conv, x_conv, y_conv,fit_data=None):\n        # Generate flat parameters\n        params = self.ffn_block(cross_conv).squeeze()\n        \n        # Split into weight and bias\n        weight = params[:self.target_in_dim * self.target_out_dim]\n        bias = torch.Tensor(params[self.target_in_dim * self.target_out_dim:]).squeeze()\n        \n        # Reshape weight into (out_dim, in_dim) for linear layer\n        weight = torch.Tensor(weight.view(-1, self.target_out_dim, self.target_in_dim)).squeeze()\n        print(weight.shape)\n        dynamic_linear = DynamicLinear(weight, bias, in_features=self.target_in_dim, out_features=self.target_out_dim)\n        \n        # fit dynamic linear\n\n        self.fit(dynamic_linear, fit_data[0], fit_data[1])\n\n        return dynamic_linear\n\n    def fit(self, model, x_input, y_label, epochs=10, tloss= 0.5):\n        # SVM loss\n        criterion = torch.nn.HingeEmbeddingLoss(margin=1.0, size_average=None, reduce=None, reduction='mean')\n        optimizer = torch.optim.SGD(model.parameters(), lr=0.01, momentum=0.9)\n        # calucate loss\n        avg_loss = 0.0\n        for i in x_input:\n            print(i.shape)\n            optimizer.zero_grad()\n            out = model(i)\n            loss = criterion(x_out, y_label)\n            loss.backward()\n            loss_ += loss.item()\n        \n        # train\n        for epoch in range(epochs):\n            for i in x_input:\n                optimizer.zero_grad()\n                out = model(i)\n                loss = criterion(x_out, y_label)\n                loss.backward()\n                optimizer.step()\n\n\n\n    \nclass Discriminator(nn.Module):\n    \"\"\"\n        Discriminate if an image is a task output or not\n        Must be fast train for each task\n        Input: \n    \"\"\"\n    def __init__(self, img_channels=10, learning_block_dim=128):\n        super(Discriminator, self).__init__()\n        # self.block1 = ParallelConvBlock(img_channels, 32)\n        # self.block2 = ParallelConvBlock(32, 64)\n        self.learning_block=128\n        self.out_channels = 64\n\n        self.cnn_block_s = nn.Sequential(\n            nn.Conv2d(img_channels, 128, kernel_size=(3, 3), padding=2),\n            nn.Conv2d(128, self.out_channels, kernel_size=(3, 3), padding=2),\n            nn.BatchNorm2d(self.out_channels),\n            nn.AdaptiveAvgPool2d((1))\n        )\n\n        self.cnn_block_w = nn.Sequential(\n            nn.Conv2d(img_channels, 128, kernel_size=(1, 3), padding=2),\n            nn.Conv2d(128, self.out_channels, kernel_size=(3, 3), padding=2),\n            nn.BatchNorm2d(self.out_channels),\n            nn.AdaptiveAvgPool2d((1))\n\n        )\n\n        self.cnn_block_h = nn.Sequential(\n            nn.Conv2d(img_channels, 128, kernel_size=(3, 1), padding=2),\n            nn.Conv2d(128, self.out_channels, kernel_size=(3, 3), padding=2),\n            nn.BatchNorm2d(self.out_channels),\n            nn.AdaptiveAvgPool2d((1))\n        )\n\n        self.cnn_block_cross = nn.Sequential(\n            nn.Conv2d(1, 256, kernel_size=(1, 2), stride=(1, 2)),\n            # nn.Conv2d(256, 256, kernel_size=(3, 1), stride(), ),\n            nn.AdaptiveAvgPool2d((1))\n        )\n\n        # GENERATOR\n        self.hyper_net = DynamicLinearGenerator(input_dim=256, params_dim=512, target_in_dim=learning_block_dim, target_out_dim=1)\n\n\n    def forward(self, train_task_inputs, train_task_outputs, target, learning_effort=10):\n        \"\"\"\n            Train\n        \"\"\"\n        # x = torch.stack([train_task_inputs == i for i in range(0, 10)]).float().unsqueeze(0) # decompose in into channels\n        # y = torch.stack([train_task_inputs == i for i in range(0, 10)]).float().unsqueeze(0) # decompose in into channels\n        # learning all the permutation of an input and an output\n        x = torch.stack([torch.roll(torch.Tensor(t), i, 1) for i in range(0, 10) for t in [torch.stack([torch.Tensor(t) == i for i in range(0, 10)]).float() for t in train_task_inputs]])\n        y = torch.stack([torch.roll(torch.Tensor(t), i, 1) for i in range(0, 10) for t in [torch.stack([torch.Tensor(t) == i for i in range(0, 10)]).float() for t in train_task_outputs]])\n        # sampling\n        # input features [N, 64]\n        x_s = self.cnn_block_s(x).squeeze()\n        x_h = self.cnn_block_h(x).squeeze()\n        x_v = self.cnn_block_w(x).squeeze()\n        \n        x_sh_concat = torch.concat([x_s, x_h, x_v], dim=1) # [N, 128]\n\n        # output features [N, 32]\n        y_s = self.cnn_block_s(y).squeeze()\n        y_h = self.cnn_block_h(y).squeeze() \n        y_v = self.cnn_block_w(y).squeeze()\n\n        y_sh_concat = torch.concat([y_s, y_h, y_v], dim=1) # [N, 128]\n        print(\"x_sh_concat\", x_sh_concat.shape)\n\n        in_out_concat = torch.stack([x_sh_concat, y_sh_concat], dim=1)\n        in_out_concat = in_out_concat.view(1, in_out_concat.shape[1]*in_out_concat.shape[0], -1)\n        in_out_pair = self.cnn_block_cross(in_out_concat)\n        \n        dynamic_linear = self.hyper_net(in_out_pair.squeeze()) # [N, 192]\n        \n        out = dynamic_linear(target)\n        \n        # task_relation, task_input_relation, task_output_relation = dynamic_linear()\n        \n        return torch.nn.Sigmoid(out)\n\n            \n        \ndiscriminator = Discriminator(10)\ntry:\n    discriminator.load_state_dict(torch.load(f'/kaggle/working/{discriminator.__class__.__name__}.pth', weights_only=True))\nexcept FileNotFoundError as e:\n    display(e)\nexcept Exception as e:\n    display(e)\n\n\n\nprint(discriminator_train_data[0][1][0][0:3])\nout = discriminator(*discriminator_train_data[0][1][0][0:3])\nprint(out)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-19T06:01:09.736992Z","iopub.execute_input":"2025-08-19T06:01:09.737459Z","iopub.status.idle":"2025-08-19T06:01:09.845062Z","shell.execute_reply.started":"2025-08-19T06:01:09.737427Z","shell.execute_reply":"2025-08-19T06:01:09.843488Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print(discriminator.eval())\nmodel_parameters = filter(lambda p: p.requires_grad, discriminator.parameters())\nparams = sum([np.prod(p.size()) for p in model_parameters])","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## train","metadata":{}},{"cell_type":"code","source":"import torch.optim as optim\n# discriminate 2 things: which image is the input and which is the output\ncriterion = nn.BCELoss()\noptimizer = optim.Adam(discriminator.parameters(), lr=0.0001)\ndef train_discriminator(epoch=1, batch_size = 500):\n    for epoch in range(epoch):\n        running_loss = 0.0\n        print(\"epoch\", epoch+1)\n        i = 0\n        for task in discriminator_train_data:\n            for sub_epoch in range(25): # learning for each different task\n                for t_inputs, t_outputs, target in task:\n                    # zero the parameter gradients\n                    optimizer.zero_grad()\n                    # get output\n                    outputs = discriminator(t_inputs, t_outputs, target)\n                    # calculate loss\n                    loss = criterion(outputs, i_label)\n                    loss.backward()\n                    # backprop\n                    optimizer.step()\n                    # print statistics\n                    running_loss += loss.item()\n                    i += 1\n                    if (i % batch_size) == (batch_size - 1):    # print every 2000 mini-batches\n                        print(outputs, i_label)\n                        print(f'[{epoch + 1}, {i + 1:5d}] loss: {running_loss / batch_size:.3f}')\n                        running_loss = 0.0\n        torch.save(discriminator.state_dict(), f'/kaggle/working/{discriminator.__class__.__name__}.pth')\n\ntrain_discriminator()\n\n\n","metadata":{"trusted":true,"scrolled":true,"execution":{"iopub.status.busy":"2025-08-17T12:19:07.640441Z","iopub.status.idle":"2025-08-17T12:19:07.640717Z","shell.execute_reply.started":"2025-08-17T12:19:07.640592Z","shell.execute_reply":"2025-08-17T12:19:07.640605Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# real training\n# train_discriminator(50)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-17T12:19:07.642601Z","iopub.status.idle":"2025-08-17T12:19:07.642851Z","shell.execute_reply.started":"2025-08-17T12:19:07.642737Z","shell.execute_reply":"2025-08-17T12:19:07.642748Z"}},"outputs":[],"execution_count":null}]}